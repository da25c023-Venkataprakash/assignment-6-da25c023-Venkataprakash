{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4a766c-a937-411f-80c7-834109d9a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa6325-1caf-4817-a754-f3f1e7ee92fa",
   "metadata": {},
   "source": [
    "## Part A.1: Load Data and Introduce Missing Values\n",
    "\n",
    "We introduce 5% Missing At Random (MAR) values in two columns:\n",
    "- **AGE**: Demographic information\n",
    "- **BILL_AMT1**: Billing amount information (one of the six billing columns only)\n",
    "\n",
    "NOTE: It was mentioned in assignment to introduce missing values in 2-3 columns, so I'm just choosing 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca341b1d-b3fb-460e-b9da-185bc0d82ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (30000, 25)\n",
      "Original missing values: 0 total missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>14027.0</td>\n",
       "      <td>13559.0</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990.0</td>\n",
       "      <td>48233.0</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>35835.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0     -2     -2     3913.0     3102.0      689.0        0.0        0.0   \n",
       "1      0      2     2682.0     1725.0     2682.0     3272.0     3455.0   \n",
       "2      0      0    29239.0    14027.0    13559.0    14331.0    14948.0   \n",
       "3      0      0    46990.0    48233.0    49291.0    28314.0    28959.0   \n",
       "4      0      0     8617.0     5670.0    35835.0    20940.0    19146.0   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0        0.0       0.0     689.0       0.0       0.0       0.0       0.0   \n",
       "1     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0   \n",
       "2    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0   \n",
       "3    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0   \n",
       "4    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0   \n",
       "\n",
       "   default.payment.next.month  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# to display all columns just for checking\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('UCI_Credit_Card.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Original missing values: {df.isna().sum().sum()} total missing values\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86097e7e-7164-4263-932a-8641060d88f5",
   "metadata": {},
   "source": [
    "**All the data points are numeric values and out target columns are age and BILL_AMT1, target in the sense the columns that we are gonna fill with nans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3c7c38-29f8-40ee-9190-e9f213954a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns count now after addding 5 percent nans AGE=1500, BILL_AMT1=1500\n"
     ]
    }
   ],
   "source": [
    "df_missing = df.copy()\n",
    "\n",
    "# making 5% missing in AGE and BILL_AMT1\n",
    "for col in ['AGE', 'BILL_AMT1']:\n",
    "    i = np.random.choice(len(df_missing), size=int(0.05 * len(df_missing)), replace=False)\n",
    "    df_missing.loc[i, col] = np.nan\n",
    "\n",
    "print(f\"Missing columns count now after addding 5 percent nans AGE={df_missing['AGE'].isna().sum()}, BILL_AMT1={df_missing['BILL_AMT1'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd501e-d56f-47d9-a7b5-2567537db28b",
   "metadata": {},
   "source": [
    "**We successfully introduced 1,500 missing values in each of the two columns (AGE and BILL_AMT1), representing 5% as required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bd443-090a-465b-b9cf-a9aad41ce984",
   "metadata": {},
   "source": [
    "## Part A.2: Strategy 1 - Median Imputation (Baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a186135-bc27-4f19-aab6-e72eb42f8e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A's Missing values after imputation AGE=0, BILL_AMT1=0\n"
     ]
    }
   ],
   "source": [
    "# Dataset A - Median Imputation\n",
    "dfA = df_missing.copy()\n",
    "dfA['AGE'].fillna(dfA['AGE'].median(), inplace=True)\n",
    "dfA['BILL_AMT1'].fillna(dfA['BILL_AMT1'].median(), inplace=True)\n",
    "\n",
    "print(f\"Dataset A's Missing values after imputation AGE={dfA['AGE'].isna().sum()}, BILL_AMT1={dfA['BILL_AMT1'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2a533-dd8e-4594-b9e7-be88b0749150",
   "metadata": {},
   "source": [
    "Median is preferred because it's not affected by outliers or skewness. In credit card data like ours now, billing amounts can have extreme values , which might make the mean unrealistically huge. Median gives us the \"middle or central\" value that better represents a typical customer, especially for skewed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d05546-d6e7-4ba8-9794-f5c26f3a08fc",
   "metadata": {},
   "source": [
    "## Part A.3: Strategy 2 - Linear Regression Imputation\n",
    "\n",
    "We use Linear Regression to predict missing AGE values based on all other features. BILL_AMT1 is taken from the original data (no missing values) as per email by TA rohit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b5cbba-62ac-4f21-a6bd-800acc26ec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset B's Missing values after imputation AGE=0\n"
     ]
    }
   ],
   "source": [
    "# Dataset B - Linear Regression (imputing AGE, and billamount1 from original data is being used as confirmed in email by rohit.)\n",
    "dfB = df_missing.copy()\n",
    "dfB['BILL_AMT1'] = df['BILL_AMT1']  # Using original bill amount as we only impute age\n",
    "\n",
    "target = 'AGE' # we want to impute this using linear regressino prediction\n",
    "drop_cols = ['ID', 'default.payment.next.month', target]\n",
    "predictors = [c for c in dfB.columns if c not in drop_cols]\n",
    "\n",
    "# differentiating known and missing, known will be used to train, and missing will be used to predict the age\n",
    "known = dfB[target].notna()\n",
    "missing = dfB[target].isna()\n",
    "\n",
    "# training data and predicting data\n",
    "X_train = dfB.loc[known, predictors]\n",
    "y_train = dfB.loc[known, target]\n",
    "X_pred = dfB.loc[missing, predictors]\n",
    "\n",
    "# using LR to predict the nans after training on non nan data\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = np.clip(lr.predict(X_pred), 18, 100).round().astype(int) # clipping weird ages\n",
    "\n",
    "#imputed \n",
    "dfB.loc[missing, target] = preds\n",
    "\n",
    "print(f\"Dataset B's Missing values after imputation AGE={dfB['AGE'].isna().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476abea4-47cd-4743-a661-931e08293323",
   "metadata": {},
   "source": [
    "In this method, a linear regression model predicts missing values in one column using other available features. We are ignoring target variable as it's dependent on input variables, and also we are assuming data is missing at random this means, that the the missing data is dependent on other variables but not on itself, which is basic definition of MAR. This linear regresion approach uses the linear relationship between other variables to predict this age which we are trying to impute here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd247f-6ac9-4a77-8193-5c32920b7067",
   "metadata": {},
   "source": [
    "## Part A.4: Strategy 3 - Non-Linear Regression (Decision Tree) [6 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c589a8-368f-4a18-9e6a-62c7db55609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset C's Missing values after imputation AGE=0\n"
     ]
    }
   ],
   "source": [
    "# Dataset C - Decision Tree \n",
    "dfC = df_missing.copy()\n",
    "dfC['BILL_AMT1'] = df['BILL_AMT1']  # Use original BILL_AMT1 and using again only age just like above one linear regression\n",
    "\n",
    "target = 'AGE' \n",
    "drop_cols = ['ID', 'default.payment.next.month', target]\n",
    "predictors = [c for c in dfC.columns if c not in drop_cols]\n",
    "\n",
    "# differentiating known and missing, known will be used to train, and missing will be used to predict the age\n",
    "\n",
    "known = dfC[target].notna()\n",
    "missing = dfC[target].isna()\n",
    "\n",
    "# training data and predicting data\n",
    "X_train = dfC.loc[known, predictors]\n",
    "y_train = dfC.loc[known, target]\n",
    "X_pred = dfC.loc[missing, predictors]\n",
    "\n",
    "# using DT to predict the nans after training on non nan data\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X_train, y_train)\n",
    "preds = np.clip(dt.predict(X_pred), 18, 100).round().astype(int)\n",
    "dfC.loc[missing, target] = preds\n",
    "\n",
    "print(f\"Dataset C's Missing values after imputation AGE={dfC['AGE'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824b141-dfcc-41da-bbf5-b16b76ed2200",
   "metadata": {},
   "source": [
    "Decision Trees can capture **non-linear relationships** and **interactions between features** without assuming linear patterns. For suppose, \n",
    "age might be related to credit limit differently for married vs single customers, decision Trees automatically discover these complex patterns\n",
    "Also this is More flexible than linear because this can capture complex relationships that linear regression would not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7addd94-8242-45c2-bb16-29d54937309b",
   "metadata": {},
   "source": [
    "# PART B: MODEL TRAINING AND PERFORMANCE ASSESSMENT\n",
    "\n",
    "Now we evaluate how our different imputation strategies affect the performance of a classification model. We'll train Logistic Regression classifiers on each dataset and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b405b-1917-49b1-bd1d-67ca83d0efa4",
   "metadata": {},
   "source": [
    "## Part B.1: Create Dataset D and Split Data [3 points]\n",
    "\n",
    "Dataset D uses listwise deletion - removing all rows with any missing values. Then we split all four datasets into 80% train and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f52ae7-7bec-4dd0-9d92-0cf9357c7597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D dataset without any null rows: 27077 rows and remember we had 30k rows\n"
     ]
    }
   ],
   "source": [
    "# Dataset D - Listwise deletion\n",
    "dfD = df_missing.dropna()\n",
    "\n",
    "print(f\"D dataset without any null rows: {len(dfD)} rows and remember we had 30k rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9afe94-a9b4-4001-bb00-af7eecebfd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "A-Median\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8179    0.9685    0.8868      4673\n",
      "           1     0.6845    0.2404    0.3558      1327\n",
      "\n",
      "    accuracy                         0.8075      6000\n",
      "   macro avg     0.7512    0.6045    0.6213      6000\n",
      "weighted avg     0.7884    0.8075    0.7694      6000\n",
      "\n",
      " Please note that results discussion will be done in part C\n",
      "\n",
      "==================================================\n",
      "B-LinearReg\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8181    0.9692    0.8873      4673\n",
      "           1     0.6897    0.2411    0.3573      1327\n",
      "\n",
      "    accuracy                         0.8082      6000\n",
      "   macro avg     0.7539    0.6052    0.6223      6000\n",
      "weighted avg     0.7897    0.8082    0.7701      6000\n",
      "\n",
      " Please note that results discussion will be done in part C\n",
      "\n",
      "==================================================\n",
      "C-DecisionTree\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8181    0.9694    0.8874      4673\n",
      "           1     0.6911    0.2411    0.3575      1327\n",
      "\n",
      "    accuracy                         0.8083      6000\n",
      "   macro avg     0.7546    0.6053    0.6225      6000\n",
      "weighted avg     0.7900    0.8083    0.7702      6000\n",
      "\n",
      " Please note that results discussion will be done in part C\n",
      "\n",
      "==================================================\n",
      "D-Listwise\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8147    0.9697    0.8855      4221\n",
      "           1     0.6735    0.2209    0.3327      1195\n",
      "\n",
      "    accuracy                         0.8045      5416\n",
      "   macro avg     0.7441    0.5953    0.6091      5416\n",
      "weighted avg     0.7835    0.8045    0.7635      5416\n",
      "\n",
      " Please note that results discussion will be done in part C\n"
     ]
    }
   ],
   "source": [
    "# Part B: Traininig and Evaluating Models for all datasets\n",
    "target_col = 'default.payment.next.month'\n",
    "drop_cols = ['ID', target_col]\n",
    "\n",
    "for name, dataset in [('A-Median', dfA), ('B-LinearReg', dfB), ('C-DecisionTree', dfC), ('D-Listwise', dfD)]:\n",
    "    X = dataset.drop(columns=drop_cols)\n",
    "    y = dataset[target_col]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(\" Please note that results discussion will be done in part C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5dd1e-ffe2-46d1-9515-cfb6172e00b0",
   "metadata": {},
   "source": [
    "# PART C: COMPARATIVE ANALYSIS\n",
    "\n",
    "Now we analyze and compare the performance of all four imputation strategies based on the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0294e9-ec3b-4638-9f78-0cdf8a4803f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Result's summary accuracy and f1 score\n",
      "================================================================================\n",
      "            Model  Overaall Accuracy   F1-Score for class1  f1-score for class0\n",
      "0        A-Median           0.807500              0.355828             0.886842\n",
      "1     B-LinearReg           0.808167              0.357342             0.887256\n",
      "2  C-DecisionTree           0.808333              0.357542             0.887365\n",
      "3      D-Listwise           0.804468              0.332703             0.885452\n"
     ]
    }
   ],
   "source": [
    "#results comparision\n",
    "target_col = 'default.payment.next.month'\n",
    "drop_cols = ['ID', target_col]\n",
    "results = []\n",
    "\n",
    "\n",
    "#same code as above in part B, but just using results list to store accracy and f1 score. kind of repitition \n",
    "for name, dataset in [('A-Median', dfA), ('B-LinearReg', dfB), ('C-DecisionTree', dfC), ('D-Listwise', dfD)]:\n",
    "    X = dataset.drop(columns=drop_cols)\n",
    "    y = dataset[target_col]\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    #logiistic regression\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #using the f1 score from the classificationreport\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    f1 = report['1']['f1-score']\n",
    "    f10 = report['0']['f1-score'] \n",
    "    results.append({'Model': name, 'Overaall Accuracy': report['accuracy'], ' F1-Score for class1': f1,'f1-score for class0' : f10})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Result's summary accuracy and f1 score\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf85f1-9066-4785-844b-2368432418bf",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "Our key focus is on  finding defaulters, so I will focus on class 1 f1 score for discussion, and class0 is almost equal for all the models if you can see the score above. Same with overall accuracy, which is same across all the models.\n",
    "\n",
    "\n",
    "Looking at the F1-scores for Class 1 :\n",
    "- Model C (Decision Tree): 0.3575 (highest), **this is a bit better than model b,estimating  that it dealt with non linear features a bit better.**\n",
    "- Model B (Linear Reg): 0.3573\n",
    "- Model A (Median): 0.3558\n",
    "- Model D (Listwise): 0.3327 (lowest)\n",
    "\n",
    "The differences between imputation methods (A, B, C) are very small (only 0.5%), but Model D (listwise deletion) performed noticeably worse, losing losing almost 7 percent of 7 percent of performance when compared to theh best,\n",
    "\n",
    "Even with the missingness being low, discarding rows actually affected the performance of the dataset for model D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53783e02-0a51-4e05-a5f3-021e9864ba03",
   "metadata": {},
   "source": [
    "## Part C.2: Discussion [10 points]\n",
    "\n",
    "**NOTE** : Our key focus is on  finding defaulters, so I will focus on class 1 for f1 score for discussion becuse it's HM of both precision and recall which are better metrics than accuracy, and class0 is almost equal for all the models if you can see the score above. Same with overall accuracy, which is same across all the models.\n",
    "\n",
    "\n",
    "### 1. Listwise Deletion vs Imputation - Trade-offs\n",
    "\n",
    "Model D (Listwise Deletion) achieved the WORST F1-score (0.3327) despite having similar overall accuracy (0.8045). This tells us the importance of dealing with null values instead of just removing them. listwise deletion.\n",
    "\n",
    "**Why deletion can perform poorly even if imputation performs worse( my imputation on age performed better, but still we are dealing with what if case)**\n",
    "- As mentioned above, we lost data which might be imporant to learn patterns and predict output, without those values, the listwise deletion actually performed **worser than median imputation** in out case, becuase we lost the data and median value atleast represents the typical user data. This tells that imputed values still keep relationship between the data, where as losing the data ignores that relationship. \n",
    "\n",
    "**Tradeoff:**\n",
    "Imputation keeps valuable information and relationships in the data, while deletion throws away patterns the model needs. Even simple median imputation outperformed deletion, showing that keeping data (even imperfectly) beats pulling it off the dataset.\n",
    "\n",
    "\n",
    "### 2. Linear vs Non-Linear Regression - Performance Comparison\n",
    "\n",
    "Model C (Decision Tree): F1 = 0.3575\n",
    "Model B (Linear Regression): F1 = 0.3573  Difference: Only 0.0002 \n",
    "\n",
    "The thing is that, they almost performed equally, with decision tree taking a little lead, so basically there are  dominating linear relationships in the data (we could've done with using corelation matrix too, but that defeats the purpose of analysis).\n",
    "\n",
    "The Decision trees might have captured non linear relationships more than LR while imputations, but the identical score tells that linear relationship is dominant in the dataset. So, I would suggest linear regression imputation only because of it's ability to score idential to nonlinear model, and also this tells us that there is linear relationship between the datafeatures too. **( if you remember this was the assumption for our linear regression earlier if you refer my LR cell above)**\n",
    "\n",
    "\n",
    "### 3. Final Recommendation\n",
    "\n",
    "I recommend Model B (Linear Regression Imputation) as the best strategy as mentioned above, although Model C scored slightly higher (0.3575 vs 0.3573).\n",
    "\n",
    "**Performance and conceptual justification:**\n",
    "- LR used all data points, with lr regression to impute the missing values introduced, and also our assumption that the missing values are dependent lineary on other columns of the data suits well with this output.\n",
    "- Decision tree performed equally well, but it mostly focuses on capturing non linear data, if it's linear data, to improve performance it might end up overfitting.\n",
    "- Median just captures the typical performance, it  doeesn't even care about the relationships in the data, and it performed well too, than the list wise.\n",
    "- List wise, with this analyis and also in different analysis if tried, just loses the relationships between the features, and ends up in losing zone.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193f954-a663-44d1-8181-8fa90c8cba50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsenv)",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
